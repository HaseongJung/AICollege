{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMuYB01EeFXpFrQP6qyxAnG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":[],"metadata":{"id":"l2MB1yh-cQ80","executionInfo":{"status":"ok","timestamp":1690530337229,"user_tz":-540,"elapsed":349,"user":{"displayName":"Andrew Wan Ju Kang","userId":"01898987751291770674"}}},"execution_count":61,"outputs":[]},{"cell_type":"code","execution_count":62,"metadata":{"id":"0648ZjfpMhmW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1690530347701,"user_tz":-540,"elapsed":10139,"user":{"displayName":"Andrew Wan Ju Kang","userId":"01898987751291770674"}},"outputId":"ef795a9e-a5e3-4640-dd9b-9a2b52396ba5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tf-agents[reverb] in /usr/local/lib/python3.10/dist-packages (0.17.0)\n","Requirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (1.4.0)\n","Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (2.2.1)\n","Requirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (0.5.0)\n","Requirement already satisfied: gym<=0.23.0,>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (0.23.0)\n","Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (1.22.4)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (9.4.0)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (1.16.0)\n","Requirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (3.20.3)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (1.14.1)\n","Requirement already satisfied: typing-extensions<4.6.0,>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (4.5.0)\n","Requirement already satisfied: pygame==2.1.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (2.1.3)\n","Requirement already satisfied: tensorflow-probability~=0.20.1 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (0.20.1)\n","Requirement already satisfied: rlds in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (0.1.8)\n","Requirement already satisfied: dm-reverb~=0.12.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (0.12.0)\n","Requirement already satisfied: tensorflow~=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (2.13.0)\n","Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from dm-reverb~=0.12.0->tf-agents[reverb]) (0.1.8)\n","Requirement already satisfied: portpicker in /usr/local/lib/python3.10/dist-packages (from dm-reverb~=0.12.0->tf-agents[reverb]) (1.5.2)\n","Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym<=0.23.0,>=0.17.0->tf-agents[reverb]) (0.0.8)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf-agents[reverb]) (1.6.3)\n","Requirement already satisfied: flatbuffers>=23.1.21 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf-agents[reverb]) (23.5.26)\n","Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf-agents[reverb]) (0.4.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf-agents[reverb]) (0.2.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf-agents[reverb]) (1.56.2)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf-agents[reverb]) (3.8.0)\n","Requirement already satisfied: keras<2.14,>=2.13.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf-agents[reverb]) (2.13.1)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf-agents[reverb]) (16.0.6)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf-agents[reverb]) (3.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf-agents[reverb]) (23.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf-agents[reverb]) (67.7.2)\n","Requirement already satisfied: tensorboard<2.14,>=2.13 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf-agents[reverb]) (2.13.0)\n","Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf-agents[reverb]) (2.13.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf-agents[reverb]) (2.3.0)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf-agents[reverb]) (0.32.0)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability~=0.20.1->tf-agents[reverb]) (4.4.2)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow~=2.13.0->tf-agents[reverb]) (0.41.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf-agents[reverb]) (2.17.3)\n","Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf-agents[reverb]) (1.0.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf-agents[reverb]) (3.4.4)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf-agents[reverb]) (2.27.1)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf-agents[reverb]) (0.7.1)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf-agents[reverb]) (2.3.6)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from portpicker->dm-reverb~=0.12.0->tf-agents[reverb]) (5.9.5)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf-agents[reverb]) (5.3.1)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf-agents[reverb]) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf-agents[reverb]) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf-agents[reverb]) (1.3.1)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf-agents[reverb]) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf-agents[reverb]) (2023.7.22)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf-agents[reverb]) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf-agents[reverb]) (3.4)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf-agents[reverb]) (2.1.3)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf-agents[reverb]) (0.5.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf-agents[reverb]) (3.2.2)\n"]}],"source":["!pip install tf-agents[reverb]"]},{"cell_type":"code","source":["import abc\n","import tensorflow as tf\n","import numpy as np\n","\n","from tf_agents.environments import py_environment\n","from tf_agents.environments import tf_environment\n","from tf_agents.environments import tf_py_environment\n","from tf_agents.environments import utils\n","from tf_agents.environments import wrappers\n","from tf_agents.environments import suite_gym\n","from tf_agents.trajectories import time_step as ts\n","from tf_agents.trajectories import trajectory     # 1월 12일\n","from tf_agents.specs import array_spec # --> obs_space, action_space"],"metadata":{"id":"8iVFfkrmOcKG","executionInfo":{"status":"ok","timestamp":1690530347703,"user_tz":-540,"elapsed":17,"user":{"displayName":"Andrew Wan Ju Kang","userId":"01898987751291770674"}}},"execution_count":63,"outputs":[]},{"cell_type":"code","source":["# 직접 환경 클래스 만들기\n","# ITA_RL_WJK_lecture_98slides 12page MDP\n","class RecyclerEnv(py_environment.PyEnvironment):\n","  def __init__(self):\n","    self._action_spec = array_spec.BoundedArraySpec(\n","        shape=(), dtype=np.int32, minimum=0, maximum=2, name='action'\n","    )\n","    # shape=() 스칼라 값 하나\n","    # 정수 타입\n","    # 최소 0; 최대 2; 0-search, 1-wait, 2-rescue\n","    # full observability일 땐 agent가 state 전체를 봅니다\n","    # 즉, state가 observation\n","    self._state_spec = array_spec.BoundedArraySpec(\n","        shape=(1,), dtype=np.int32, minimum=0, maximum=1, name='observation'\n","    )\n","    self._state = 1\n","    self._done = False\n","    self._gamma = 0.99\n","    self._alpha = 0.7\n","    self._beta = 0.95\n","    # 상태가 LOW인데 critical번 search를 나가면 종료(done=True)\n","    self._low = 0\n","    self._critical = 5\n","\n","    self._reward = 0    # cans\n","    return\n","\n","  def action_spec(self):\n","    return self._action_spec\n","\n","  def observation_spec(self):\n","    return self._state_spec\n","\n","  def _reset(self):\n","    self._state = 1     # 1=충전된 채로 시작\n","                        # 확률적으로 0 또는 1을 반환해도 됨\n","    self._done = False\n","    self._low = 0\n","    self._reward = 0\n","    return ts.restart(np.array([self._state], dtype=np.int32))\n","\n","  # 이론적으론 step 함수만 있어도 됨\n","  # 위의 다른 함수들은 코드에서 손실/기울기 등 계산을 하기 위해\n","  # 또는 action/state 차원(모양)을 알기 위해-->신경망 입출력단 설계에 쓰임\n","  def _step(self, action):\n","    if self._reward > 100:\n","      self._done = True\n","    if self._done:\n","      return self._reset()\n","\n","    # action마다 무슨 일이 일어날지\n","    if action == 0:   # search\n","      # 확률에 따른 상태 이동 구현 방법\n","      # self._state = np.random.choice(2, 1, p=[self._beta, 1-self._beta])\n","      # 2: 0 또는 1이 출력됨\n","      # 1: 난수 몇개 생성할지\n","      # p: 0이 나올 확률, 1이 나올 확률\n","      p = np.random.rand()    # uniform distribution[0, 1]\n","      if self._state == 0:    # low에서 search\n","        self._low += 1\n","        if (self._low > self._critical): # low에서 search를 너무 많이 나갔을 때\n","          self._done = True\n","          reward = -10.0\n","          self._reward += reward   # 1월 12일\n","          return ts.termination(np.array([self._state], dtype=np.int32), reward)\n","        if p < self._beta:    # low에서 search (beta)\n","          self._state = 0\n","          reward = 1\n","        else:                 # low에서 search (1-beta)\n","          self._state = 1\n","          reward = -3\n","      else:                   # high에서 search\n","        #self._state = np.random.choice(2, 1, p=[1.0 - self._alpha, self._alpha])\n","        #reward = np.random.choice([1, 1], p=[1.0-self._alpha, self._alpha])\n","        sp_and_r = np.array([[0, 1], [1, 1]])\n","        idx = np.random.choice(len(sp_and_r),\n","                              p=[1.0-self._alpha,self._alpha])\n","        self._state, reward = sp_and_r[idx][0], sp_and_r[idx][1]\n","      # search했을 때 함수 호출 한번으로 상태 이동 정의\n","      self._reward += reward       # 1월 12일\n","      return ts.transition(np.array([self._state], dtype=np.int32),\n","                           reward, discount=self._gamma)\n","    elif action == 1: # wait\n","      if self._state == 0:\n","        self._state = 0\n","        reward = -0.1\n","      else:\n","        self._state = 0\n","        reward = -0.1\n","      self._reward += reward       # 1월 12일\n","      return ts.transition(np.array([self._state], dtype=np.int32),\n","                    reward, discount=self._gamma)\n","    else:             # recharge\n","      self._done = False\n","      if self._state == 0:\n","        self._state = 1\n","\n","      reward = 0.0\n","      # transition(상태 이동)\n","      self._reward += reward       # 1월 12일\n","      return ts.transition(\n","          np.array([self._state], dtype=np.int32), reward, discount=self._gamma)"],"metadata":{"id":"io-U94wxPbwh","executionInfo":{"status":"ok","timestamp":1690530348267,"user_tz":-540,"elapsed":22,"user":{"displayName":"Andrew Wan Ju Kang","userId":"01898987751291770674"}}},"execution_count":64,"outputs":[]},{"cell_type":"code","source":["_env = RecyclerEnv()"],"metadata":{"id":"D2PXHrQtjZt0","executionInfo":{"status":"ok","timestamp":1690530348268,"user_tz":-540,"elapsed":22,"user":{"displayName":"Andrew Wan Ju Kang","userId":"01898987751291770674"}}},"execution_count":65,"outputs":[]},{"cell_type":"code","source":["utils.validate_py_environment(_env, episodes=5)"],"metadata":{"id":"DAzQJtjfknm3","executionInfo":{"status":"ok","timestamp":1690530348269,"user_tz":-540,"elapsed":21,"user":{"displayName":"Andrew Wan Ju Kang","userId":"01898987751291770674"}}},"execution_count":66,"outputs":[]},{"cell_type":"code","source":["recycler_env = tf_py_environment.TFPyEnvironment(_env)"],"metadata":{"id":"l_HEdagnwBv_","executionInfo":{"status":"ok","timestamp":1690530348269,"user_tz":-540,"elapsed":20,"user":{"displayName":"Andrew Wan Ju Kang","userId":"01898987751291770674"}}},"execution_count":67,"outputs":[]},{"cell_type":"code","source":["# TF-agents DQN을 가져와서 위 환경에서 학습\n","from tf_agents.agents.dqn import dqn_agent\n","from tf_agents.eval import metric_utils\n","from tf_agents.metrics import tf_metrics\n","from tf_agents.networks import sequential\n","from tf_agents.policies import random_tf_policy\n","from tf_agents.policies import py_tf_eager_policy\n","from tf_agents.replay_buffers import reverb_replay_buffer\n","from tf_agents.replay_buffers import reverb_utils\n","from tf_agents.specs import tensor_spec\n","from tf_agents.utils import common"],"metadata":{"id":"EXDLi44HocKs","executionInfo":{"status":"ok","timestamp":1690530348270,"user_tz":-540,"elapsed":19,"user":{"displayName":"Andrew Wan Ju Kang","userId":"01898987751291770674"}}},"execution_count":68,"outputs":[]},{"cell_type":"code","source":["# 학습 파라미터\n","# 강화학습 특정 하이퍼파라미터\n","num_iter = 10000\n","random_episodes = 50\n","collect_per_iteration = 1\n","replay_buffer_size = 100\n","n_step_update = 2\n","\n","# 강화학습 외의 구성 요소 (딥러닝) 하이퍼파라미터\n","batch_size = 33\n","learning_rate = 0.001\n","log_interval = 200\n","\n","num_eval_episodes = 10      # evaluation을 몇 episode에 걸쳐서 할지\n","eval_interval = 100         # evaluation 몇 에피소드마다 할지"],"metadata":{"id":"t0mgqm-npzYW","executionInfo":{"status":"ok","timestamp":1690530348271,"user_tz":-540,"elapsed":20,"user":{"displayName":"Andrew Wan Ju Kang","userId":"01898987751291770674"}}},"execution_count":69,"outputs":[]},{"cell_type":"code","source":["# TF-agents DQN\n","fc_layer_params = (16, 32, 8)        # Dense layer 폭들의 어레이\n","\n","action_tensor_spec = tensor_spec.from_spec(recycler_env.action_spec())\n","num_actions = action_tensor_spec.maximum - action_tensor_spec.minimum + 1\n","# num_actions # search, wait, rescue\n","\n","def dense_layer(num_units):\n","  return tf.keras.layers.Dense(\n","      num_units,\n","      activation=\"relu\")\n","\n","dense_layers = [dense_layer(num_units) for num_units in fc_layer_params]\n","q_values_layer = tf.keras.layers.Dense(\n","    num_actions,\n","    activation=None)\n","\n","deepqn = sequential.Sequential(dense_layers + [q_values_layer])\n","# keras Sequential은 인자로 리스트를 받습니다\n","# [list1] + [list2] = [list1, list2]\n","# 리스트의 원소는 각각 layer [Dense(), Dense(), Dense(), Dense(num_actions)]"],"metadata":{"id":"pPPGN7HpqxX3","executionInfo":{"status":"ok","timestamp":1690530348271,"user_tz":-540,"elapsed":20,"user":{"displayName":"Andrew Wan Ju Kang","userId":"01898987751291770674"}}},"execution_count":70,"outputs":[]},{"cell_type":"code","source":["optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n","train_step = tf.Variable(0)   # 지금 몇번째 스텝인지 추적하기 위한 0\n","\n","\n","### TF-Agents DQN ###\n","agent = dqn_agent.DqnAgent(\n","    recycler_env.time_step_spec(),\n","    recycler_env.action_spec(),\n","    q_network=deepqn,\n","    optimizer=optimizer,\n","    n_step_update=n_step_update,\n","    td_errors_loss_fn=common.element_wise_squared_loss,\n","    train_step_counter= train_step)\n","\n","agent.initialize()"],"metadata":{"id":"wmHvuzsUtJqj","executionInfo":{"status":"ok","timestamp":1690530348271,"user_tz":-540,"elapsed":19,"user":{"displayName":"Andrew Wan Ju Kang","userId":"01898987751291770674"}}},"execution_count":71,"outputs":[]},{"cell_type":"code","source":["# 할일\n","# Replay Buffer\n","# Average Return 계산\n","# PyDriver로 루프 --> for loop 돌기에 최적화된 클래스\n","# 데이터 축적\n","def avg_return(env, policy, num_episodes=10):\n","  # print(env.__dir__())\n","  # print(type(env))\n","  # print(env._reset().is_last())\n","  total_return = 0.0 # 누적 합\n","  time_step = env._reset()\n","  # done = env._done\n","  for _ in range(num_episodes):\n","    time_step = env._reset()\n","    episode_return = 0.0\n","\n","    #while not time_step.is_last(): # 이 에피소드의 마지막 타임스텝까지\n","    for _ in range(10):\n","\n","      action_step = policy.action(time_step)\n","      time_step = env._step(action_step.action)\n","      episode_return += time_step.reward # 보상들의 합\n","      # done = env._done\n","    total_return += episode_return\n","    time_step = env._reset()\n","  avg = total_return / num_episodes\n","  return avg.numpy()[0]"],"metadata":{"id":"VAxldPPqwQTv","executionInfo":{"status":"ok","timestamp":1690530348272,"user_tz":-540,"elapsed":20,"user":{"displayName":"Andrew Wan Ju Kang","userId":"01898987751291770674"}}},"execution_count":72,"outputs":[]},{"cell_type":"code","source":["# 1월 12일\n","from tf_agents.replay_buffers import tf_uniform_replay_buffer"],"metadata":{"id":"nI1cpln9LLkX","executionInfo":{"status":"ok","timestamp":1690530348611,"user_tz":-540,"elapsed":358,"user":{"displayName":"Andrew Wan Ju Kang","userId":"01898987751291770674"}}},"execution_count":73,"outputs":[]},{"cell_type":"code","source":["replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n","    data_spec=agent.collect_data_spec,\n","    batch_size=recycler_env.batch_size,\n","    max_length=replay_buffer_size)\n","\n","random_policy = random_tf_policy.RandomTFPolicy(recycler_env.time_step_spec(),\n","                                                recycler_env.action_spec())\n","\n","def collect_step(environment, policy):\n","  time_step = environment.current_time_step()\n","  action_step = policy.action(time_step)\n","  next_time_step = environment.step(action_step.action)\n","  traj = trajectory.from_transition(time_step, action_step, next_time_step)\n","\n","  # Add trajectory to the replay buffer\n","  replay_buffer.add_batch(traj)\n","\n","for _ in range(random_episodes):\n","  collect_step(recycler_env, random_policy)"],"metadata":{"id":"tBPrFDF90G_N","executionInfo":{"status":"ok","timestamp":1690530353115,"user_tz":-540,"elapsed":19,"user":{"displayName":"Andrew Wan Ju Kang","userId":"01898987751291770674"}}},"execution_count":74,"outputs":[]},{"cell_type":"code","source":["dataset = replay_buffer.as_dataset(\n","    num_parallel_calls=3, sample_batch_size=batch_size,\n","    num_steps=n_step_update+1).prefetch(3)\n","\n","iterator = iter(dataset) # 버퍼에서 샘플링한 경험을 순회하는 객체"],"metadata":{"id":"XDw8x4M_MmCY","executionInfo":{"status":"ok","timestamp":1690530550104,"user_tz":-540,"elapsed":1415,"user":{"displayName":"Andrew Wan Ju Kang","userId":"01898987751291770674"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"4f79f212-ddfb-4916-90ab-2074ecbe5a48"},"execution_count":77,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tf_agents/replay_buffers/tf_uniform_replay_buffer.py:342: CounterV2 (from tensorflow.python.data.experimental.ops.counter) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.data.Dataset.counter(...)` instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py:377: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `as_dataset(..., single_deterministic_pass=False) instead.\n"]}]},{"cell_type":"code","source":["agent.train_step_counter.assign(0)\n","\n","avg_return_value = avg_return(recycler_env, agent.policy, num_eval_episodes)\n","returns = [avg_return_value]\n","\n","for _ in range(num_iter):\n","\n","  # Collect a few steps using collect_policy and save to the replay buffer.\n","  for _ in range(collect_per_iteration):\n","    collect_step(recycler_env, agent.collect_policy)\n","\n","  # Sample a batch of data from the buffer and update the agent's network.\n","  experience, unused_info = next(iterator) # 그 다음 경험에 대한 학습\n","  train_loss = agent.train(experience) # (s, a, r, s')\n","\n","  step = agent.train_step_counter.numpy()\n","\n","  if step % log_interval == 0:\n","    print('step = {0}: loss = {1}'.format(step, train_loss.loss))\n","\n","  if step % eval_interval == 0:\n","    avg_return_value = avg_return(recycler_env, agent.policy, num_eval_episodes)\n","    print('step = {0}: Average Return = {1:.2f}'.format(step, avg_return_value))\n","    returns.append(avg_return_value)"],"metadata":{"id":"J8beXgYyM4D3","executionInfo":{"status":"error","timestamp":1690530753955,"user_tz":-540,"elapsed":201221,"user":{"displayName":"Andrew Wan Ju Kang","userId":"01898987751291770674"}},"outputId":"0da34df7-816d-425c-8614-c64ab69cf2f9","colab":{"base_uri":"https://localhost:8080/","height":1000}},"execution_count":78,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1176: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\n","Instructions for updating:\n","back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n","Instead of:\n","results = tf.foldr(fn, elems, back_prop=False)\n","Use:\n","results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n","WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7b56a695b370> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7b56a695b370> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"]},{"output_type":"stream","name":"stdout","text":["step = 100: Average Return = 2.60\n","step = 200: loss = 20.205564498901367\n","step = 200: Average Return = 7.70\n","step = 300: Average Return = 8.50\n","step = 400: loss = 42.7346076965332\n","step = 400: Average Return = 0.00\n","step = 500: Average Return = 0.00\n","step = 600: loss = 6.958929538726807\n","step = 600: Average Return = 0.00\n","step = 700: Average Return = 7.80\n","step = 800: loss = 7.691587448120117\n","step = 800: Average Return = 8.10\n","step = 900: Average Return = 7.50\n","step = 1000: loss = 1.3839343786239624\n","step = 1000: Average Return = -0.50\n","step = 1100: Average Return = 7.70\n","step = 1200: loss = 0.6158702969551086\n","step = 1200: Average Return = 7.60\n","step = 1300: Average Return = 7.80\n","step = 1400: loss = 0.24681100249290466\n","step = 1400: Average Return = 7.70\n","step = 1500: Average Return = -0.50\n","step = 1600: loss = 0.20735685527324677\n","step = 1600: Average Return = 7.70\n","step = 1700: Average Return = 8.10\n","step = 1800: loss = 0.34770673513412476\n","step = 1800: Average Return = 8.20\n","step = 1900: Average Return = 8.00\n","step = 2000: loss = 0.37198513746261597\n","step = 2000: Average Return = 8.10\n","step = 2100: Average Return = 7.50\n","step = 2200: loss = 0.20856954157352448\n","step = 2200: Average Return = 7.70\n","step = 2300: Average Return = 8.10\n","step = 2400: loss = 0.1999237984418869\n","step = 2400: Average Return = 8.20\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-78-81a39caa4cb2>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0;31m# Sample a batch of data from the buffer and update the agent's network.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0mexperience\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munused_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 그 다음 경험에 대한 학습\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m   \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperience\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (s, a, r, s')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tf_agents/agents/tf_agent.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, experience, weights, **kwargs)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_functions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m       loss_info = self._train_fn(\n\u001b[0m\u001b[1;32m    331\u001b[0m           experience=experience, weights=weights, **kwargs)\n\u001b[1;32m    332\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tf_agents/utils/common.py\u001b[0m in \u001b[0;36mwith_check_resource_vars\u001b[0;34m(*fn_args, **fn_kwargs)\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;31m# We're either in eager mode or in tf.function mode (no in-between); so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;31m# autodep-like behavior is already expected of fn.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresource_variables_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMISSING_RESOURCE_VARIABLES_ERROR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tf_agents/agents/dqn/dqn_agent.py\u001b[0m in \u001b[0;36m_train\u001b[0;34m(self, experience, weights)\u001b[0m\n\u001b[1;32m    391\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperience\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m       loss_info = self._loss(\n\u001b[0m\u001b[1;32m    394\u001b[0m           \u001b[0mexperience\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m           \u001b[0mtd_errors_loss_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_td_errors_loss_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tf_agents/agents/dqn/dqn_agent.py\u001b[0m in \u001b[0;36m_loss\u001b[0;34m(self, experience, td_errors_loss_fn, gamma, reward_scale_factor, weights, training)\u001b[0m\n\u001b[1;32m    466\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mnumber\u001b[0m \u001b[0mof\u001b[0m \u001b[0mactions\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mgreater\u001b[0m \u001b[0mthan\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m     \"\"\"\n\u001b[0;32m--> 468\u001b[0;31m     \u001b[0mtransition\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_transition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperience\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m     \u001b[0mtime_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_time_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_steps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tf_agents/agents/data_converter.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    597\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrajectory_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           sequence_length=None if self._n is None else self._n + 1)\n\u001b[0;32m--> 599\u001b[0;31m       \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrajectory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_n_step_transition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Input type not supported: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tf_agents/trajectories/trajectory.py\u001b[0m in \u001b[0;36mto_n_step_transition\u001b[0;34m(trajectory, gamma)\u001b[0m\n\u001b[1;32m    750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m   \u001b[0;31m# Pull out x[:,-1] for x in trajectory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 752\u001b[0;31m   final_frame = tf.nest.map_structure(\n\u001b[0m\u001b[1;32m    753\u001b[0m       lambda t: composite.squeeze(\n\u001b[1;32m    754\u001b[0m           \u001b[0mcomposite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslice_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mwrong\u001b[0m \u001b[0mkeyword\u001b[0m \u001b[0marguments\u001b[0m \u001b[0mare\u001b[0m \u001b[0mprovided\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m   \"\"\"\n\u001b[0;32m--> 624\u001b[0;31m   return nest_util.map_structure(\n\u001b[0m\u001b[1;32m    625\u001b[0m       \u001b[0mnest_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModality\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCORE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m   )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/nest_util.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(modality, func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m   1052\u001b[0m   \"\"\"\n\u001b[1;32m   1053\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmodality\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mModality\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCORE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1054\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_tf_core_map_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1055\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mmodality\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mModality\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDATA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_tf_data_map_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/nest_util.py\u001b[0m in \u001b[0;36m_tf_core_map_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m   1092\u001b[0m   return _tf_core_pack_sequence_as(\n\u001b[1;32m   1093\u001b[0m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1094\u001b[0;31m       \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1095\u001b[0m       \u001b[0mexpand_composites\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexpand_composites\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1096\u001b[0m   )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/nest_util.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1092\u001b[0m   return _tf_core_pack_sequence_as(\n\u001b[1;32m   1093\u001b[0m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1094\u001b[0;31m       \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1095\u001b[0m       \u001b[0mexpand_composites\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexpand_composites\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1096\u001b[0m   )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tf_agents/trajectories/trajectory.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    752\u001b[0m   final_frame = tf.nest.map_structure(\n\u001b[1;32m    753\u001b[0m       lambda t: composite.squeeze(\n\u001b[0;32m--> 754\u001b[0;31m           \u001b[0mcomposite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslice_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m           axis=1),\n\u001b[1;32m    756\u001b[0m       trajectory)\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tf_agents/utils/composite.py\u001b[0m in \u001b[0;36mslice_from\u001b[0;34m(tensor, axis, start)\u001b[0m\n\u001b[1;32m    157\u001b[0m                    \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m                    + [slice(None)] * (ndims - axis - 1))\n\u001b[0;32m--> 159\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mslices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1174\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1176\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1177\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36m_slice_helper\u001b[0;34m(tensor, slice_spec, var)\u001b[0m\n\u001b[1;32m   1116\u001b[0m       \u001b[0mvar_empty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1117\u001b[0m       \u001b[0mpacked_begin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpacked_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpacked_strides\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvar_empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1118\u001b[0;31m     return strided_slice(\n\u001b[0m\u001b[1;32m   1119\u001b[0m         \u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1120\u001b[0m         \u001b[0mpacked_begin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1174\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1176\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1177\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mstrided_slice\u001b[0;34m(input_, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, var, name)\u001b[0m\n\u001b[1;32m   1289\u001b[0m     \u001b[0mstrides\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1291\u001b[0;31m   op = gen_array_ops.strided_slice(\n\u001b[0m\u001b[1;32m   1292\u001b[0m       \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1293\u001b[0m       \u001b[0mbegin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mstrided_slice\u001b[0;34m(input, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, name)\u001b[0m\n\u001b[1;32m  10695\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10696\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 10697\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m  10698\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"StridedSlice\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbegin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"begin_mask\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10699\u001b[0m         \u001b[0mbegin_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"end_mask\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ellipsis_mask\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mellipsis_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}