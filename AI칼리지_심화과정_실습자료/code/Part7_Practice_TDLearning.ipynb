{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNb9ZEnY0NotwkARsFKDaUs"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"Ck99Pvv7Ddk4","executionInfo":{"status":"ok","timestamp":1690540458604,"user_tz":-540,"elapsed":3,"user":{"displayName":"Andrew Wan Ju Kang","userId":"01898987751291770674"}}},"outputs":[],"source":["# Q-learning이랑 SARSA를 tabular learning으로 구현\n","# 47번, 48번 슬라이드 참고"]},{"cell_type":"code","source":["import gym\n","import random # 탐사하는 데 쓰기\n","from collections import namedtuple # Q-table 만드는 데 쓸 자료구조\n","import collections\n","import numpy as np\n","import matplotlib.pyplot as plt"],"metadata":{"id":"AOg2zEdsED5G","executionInfo":{"status":"ok","timestamp":1690541071790,"user_tz":-540,"elapsed":513,"user":{"displayName":"Andrew Wan Ju Kang","userId":"01898987751291770674"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# 함수 세개 구현\n","# 1. epsilon-greedy 액션 선택기 구현\n","# 2. greedy 액션 선택기 구현 (테스트 때)\n","# 3. 최적 액션 선택기 (1이랑 2에서 쓰임)"],"metadata":{"id":"wDDMKfetGZdX","executionInfo":{"status":"ok","timestamp":1690541177607,"user_tz":-540,"elapsed":2,"user":{"displayName":"Andrew Wan Ju Kang","userId":"01898987751291770674"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["def eps_greedy_action(table, state):\n","  value, action = best_value_and_action(table, state)\n","  if random.random() < epsilon:\n","    # 탐사\n","    return random.randint(0, n_actions-1)\n","  else:\n","    # 활용\n","    return action"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"duWNK7MCGziK","executionInfo":{"status":"ok","timestamp":1690545165911,"user_tz":-540,"elapsed":310,"user":{"displayName":"Andrew Wan Ju Kang","userId":"01898987751291770674"}},"outputId":"8528e203-e055-43e8-b211-c90764b31bad"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]}]},{"cell_type":"code","source":["def greedy_action(table, state):\n","  value, action = best_value_and_action(table, state)\n","  return action"],"metadata":{"id":"WJDWy9YiHHYD","executionInfo":{"status":"ok","timestamp":1690543007199,"user_tz":-540,"elapsed":514,"user":{"displayName":"Andrew Wan Ju Kang","userId":"01898987751291770674"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["def best_value_and_action(table, state):\n","  best_value = 0\n","  best_action = 0\n","  # 지금 가능한 액션 선택지 모두를 테이블에서 돌면서 가장 높은 값과 그 때의 액션 반환\n","  for action in range(n_actions):\n","    if table[(state, action)] > best_value:\n","      best_action = action\n","      best_value = table[(state, action)] # Q(s, a)\n","  return best_value, best_action"],"metadata":{"id":"SMmrLR2zHML2","executionInfo":{"status":"ok","timestamp":1690543008211,"user_tz":-540,"elapsed":1,"user":{"displayName":"Andrew Wan Ju Kang","userId":"01898987751291770674"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# 47번 슬라이드\n","# SARSA 구현\n","# 알고리즘 내용에서 가장 안쪽 루프의 안 내용\n","def SARSA(table, state, action, reward, state_next):\n","  action_next = eps_greedy_action(table, state_next)\n","  Q_target = reward + GAMMA * table[(state_next, action_next)]\n","  TD_error = Q_target - table[(state, action)]\n","  table[(state, action)] = table[(state, action)] + LEARNING_RATE * TD_error\n","  return"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QYveCR2zNzb6","executionInfo":{"status":"ok","timestamp":1690545105572,"user_tz":-540,"elapsed":8,"user":{"displayName":"Andrew Wan Ju Kang","userId":"01898987751291770674"}},"outputId":"1d3333f0-2940-44c8-e4f3-b77aeed46707"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]}]},{"cell_type":"code","source":["# 48번 슬라이드\n","# Q-learning 구현\n","# 알고리즘 내용에서 가장 안쪽 루프의 안 내용\n","def Q_learn(table, state, action, reward, state_next):\n","  best_value, _ = best_value_and_action(table, state_next)\n","  Q_target = reward + GAMMA * best_value # max_a(Q(S', a))\n","  TD_error = Q_target - table[(state, action)]\n","  table[(state, action)] = table[(state, action)] + LEARNING_RATE * TD_error\n","\n","  return"],"metadata":{"id":"XKlbnVqdTfUu","executionInfo":{"status":"ok","timestamp":1690545115373,"user_tz":-540,"elapsed":324,"user":{"displayName":"Andrew Wan Ju Kang","userId":"01898987751291770674"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["# 테스트 함수\n","# 이전에 model.evaluate\n","def test(env, table):\n","  reward_games = [] # 이 리스트에 보상 값 저장해놓고 평균 구할 거임\n","  for _ in range(TEST_EPISODES):\n","    state_ = env.reset()\n","    rewards = 0\n","    while True:\n","      next_state_, reward, done, _ = env.step(greedy_action(table, state_))\n","      state_ = next_state_\n","      rewards += reward # 이번 게임 누적 합산 보상\n","\n","      if done:\n","        reward_games.append(rewards) # 모든 게임들 누적 합산 보상 값이 저장되어 있는 리스트\n","        break\n","  return np.mean(reward_games)"],"metadata":{"id":"hw0qYlq-OKuy","executionInfo":{"status":"ok","timestamp":1690544497410,"user_tz":-540,"elapsed":4,"user":{"displayName":"Andrew Wan Ju Kang","userId":"01898987751291770674"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["seed = 42\n","random.seed(seed)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8Yo9x8yTOUmD","executionInfo":{"status":"ok","timestamp":1690543164960,"user_tz":-540,"elapsed":2,"user":{"displayName":"Andrew Wan Ju Kang","userId":"01898987751291770674"}},"outputId":"aa9ee81c-9d31-4c25-c934-fa7b37252095"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]}]},{"cell_type":"code","source":["# 강화학습 관련 하이퍼파라미터\n","GAMMA = 0.9 # discount factor\n","# 환경에 의해 미리 정해진 값일 수도 있음\n","# 만약 그렇지 않다면 강화학습 모델 개발자의 자유\n","EPS_DECAY = 0.999 # epsilon decay?\n","# epsilon 값을 점점 작게\n","# 왜? 처음에는 탐사에 비중을 더 두고, 나중으로 갈수록 활용의 비중을 높이기 위해\n","LEARNING_RATE = 0.8 # 슬라이드에서 alpha\n","MAX_GAMES = 50000 # 5만 에피소드에 걸쳐 학습하기\n","\n","PRINT_EVERY = 1000 # 1천번 에피소드마다 로그 보기\n","TEST_EPISODES = 100 # 위의 테스트 함수에서 플레이할 에피소드 갯수"],"metadata":{"id":"kmBwEeLJOYrs","executionInfo":{"status":"ok","timestamp":1690543407140,"user_tz":-540,"elapsed":2,"user":{"displayName":"Andrew Wan Ju Kang","userId":"01898987751291770674"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# 환경 소환\n","env = gym.make(\"FrozenLake-v1\", is_slippery=True)\n","state_ = env.reset() # 초기화 에피소드 시작하기 직전에\n","state_length = env.observation_space.n\n","n_actions = env.action_space.n\n","\n","reward_cnt = 0\n","game_cnt = 0 # 여태 몇번의 에피소드를 플레이했는지\n","\n","# Q-table\n","table_ = collections.defaultdict(float) # Q 값으로 채워질 테이블\n","\n","# 테스트에서 얻은 보상들 리스트\n","test_rewards = []\n","\n","epsilon = 1.0 # 처음엔 탐사만 하겠다 ---> EPS_DECAY를 여기에 계속 곱해서 서서히 줄어들게 할 거임\n","\n","while game_cnt < MAX_GAMES:\n","  action = eps_greedy_action(table_, state_) # behavior rule (Q-learning이든 SARSA든 공통임)\n","  next_state_, reward, done, _ = env.step(action) # 한번 스텝은 강화학습 사이클 한번\n","  # next_state_: 다음 상태\n","  # reward: 보상\n","  # done: True/False boolean 지금 task가 끝났는지\n","  # _: info를 반환하도록 구현된 환경들이 있음; 근데 지금 우리는 안 씀\n","\n","  Q_learn(table_, state_, action, reward, next_state_)\n","  # SARSA(table_, state_, action, reward, next_state_)\n","  reward_cnt += reward # 누적 합산 보상\n","  state_ = next_state_\n","\n","  # 만약 done이 True면?\n","  if done:\n","    epsilon *= EPS_DECAY\n","    state_ = env.reset()\n","    reward_cnt = 0\n","    game_cnt += 1\n","\n","    if ((game_cnt + 1) % PRINT_EVERY) == 0:\n","      test_reward = test(env, table_)\n","      test_rewards.append(test_reward)\n","      print(\"현재 {}번째 에피소드 학습 중... {}개 에피소드 평균 보상: {}\".format(game_cnt, TEST_EPISODES, test_reward))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"gG122PBmPTvy","executionInfo":{"status":"error","timestamp":1690545227388,"user_tz":-540,"elapsed":55165,"user":{"displayName":"Andrew Wan Ju Kang","userId":"01898987751291770674"}},"outputId":"9ad557cc-30a6-4972-9f2f-49f4f6f1e73c"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n"]},{"output_type":"stream","name":"stdout","text":["현재 999번째 에피소드 학습 중... 100개 에피소드 평균 보상: 0.17\n","현재 1999번째 에피소드 학습 중... 100개 에피소드 평균 보상: 0.51\n","현재 2999번째 에피소드 학습 중... 100개 에피소드 평균 보상: 0.5\n","현재 3999번째 에피소드 학습 중... 100개 에피소드 평균 보상: 0.75\n","현재 4999번째 에피소드 학습 중... 100개 에피소드 평균 보상: 0.69\n","현재 5999번째 에피소드 학습 중... 100개 에피소드 평균 보상: 0.69\n","현재 6999번째 에피소드 학습 중... 100개 에피소드 평균 보상: 0.54\n","현재 7999번째 에피소드 학습 중... 100개 에피소드 평균 보상: 0.51\n","현재 8999번째 에피소드 학습 중... 100개 에피소드 평균 보상: 0.69\n","현재 9999번째 에피소드 학습 중... 100개 에피소드 평균 보상: 0.61\n","현재 10999번째 에피소드 학습 중... 100개 에피소드 평균 보상: 0.69\n","현재 11999번째 에피소드 학습 중... 100개 에피소드 평균 보상: 0.79\n","현재 12999번째 에피소드 학습 중... 100개 에피소드 평균 보상: 0.73\n","현재 13999번째 에피소드 학습 중... 100개 에피소드 평균 보상: 0.75\n","현재 14999번째 에피소드 학습 중... 100개 에피소드 평균 보상: 0.84\n","현재 15999번째 에피소드 학습 중... 100개 에피소드 평균 보상: 0.56\n","현재 16999번째 에피소드 학습 중... 100개 에피소드 평균 보상: 0.65\n","현재 17999번째 에피소드 학습 중... 100개 에피소드 평균 보상: 0.76\n","현재 18999번째 에피소드 학습 중... 100개 에피소드 평균 보상: 0.74\n","현재 19999번째 에피소드 학습 중... 100개 에피소드 평균 보상: 0.77\n","현재 20999번째 에피소드 학습 중... 100개 에피소드 평균 보상: 0.74\n","현재 21999번째 에피소드 학습 중... 100개 에피소드 평균 보상: 0.69\n","현재 22999번째 에피소드 학습 중... 100개 에피소드 평균 보상: 0.7\n","현재 23999번째 에피소드 학습 중... 100개 에피소드 평균 보상: 0.67\n","현재 24999번째 에피소드 학습 중... 100개 에피소드 평균 보상: 0.73\n","현재 25999번째 에피소드 학습 중... 100개 에피소드 평균 보상: 0.68\n","현재 26999번째 에피소드 학습 중... 100개 에피소드 평균 보상: 0.78\n","현재 27999번째 에피소드 학습 중... 100개 에피소드 평균 보상: 0.75\n","현재 28999번째 에피소드 학습 중... 100개 에피소드 평균 보상: 0.7\n","현재 29999번째 에피소드 학습 중... 100개 에피소드 평균 보상: 0.7\n","현재 30999번째 에피소드 학습 중... 100개 에피소드 평균 보상: 0.75\n","현재 31999번째 에피소드 학습 중... 100개 에피소드 평균 보상: 0.75\n","현재 32999번째 에피소드 학습 중... 100개 에피소드 평균 보상: 0.78\n","현재 33999번째 에피소드 학습 중... 100개 에피소드 평균 보상: 0.5\n","현재 34999번째 에피소드 학습 중... 100개 에피소드 평균 보상: 0.57\n","현재 35999번째 에피소드 학습 중... 100개 에피소드 평균 보상: 0.58\n","현재 36999번째 에피소드 학습 중... 100개 에피소드 평균 보상: 0.58\n","현재 37999번째 에피소드 학습 중... 100개 에피소드 평균 보상: 0.53\n","현재 38999번째 에피소드 학습 중... 100개 에피소드 평균 보상: 0.55\n","현재 39999번째 에피소드 학습 중... 100개 에피소드 평균 보상: 0.48\n","현재 40999번째 에피소드 학습 중... 100개 에피소드 평균 보상: 0.77\n","현재 41999번째 에피소드 학습 중... 100개 에피소드 평균 보상: 0.77\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-20-1c59679af40d>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mgame_cnt\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mMAX_GAMES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meps_greedy_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# behavior rule (Q-learning이든 SARSA든 공통임)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m   \u001b[0mnext_state_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 한번 스텝은 강화학습 사이클 한번\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m   \u001b[0;31m# next_state_: 다음 상태\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0;31m# reward: 보상\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \"\"\"\n\u001b[1;32m     59\u001b[0m         observation, reward, terminated, truncated, info = step_api_compatibility(\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m             \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/wrappers/order_enforcing.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mResetNeeded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot call env.step() before calling env.reset()\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \"\"\"\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mstep_returns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_step_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstep_to_new_api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_returns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/wrappers/env_checker.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchecked_render\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mActType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0;34m\"\"\"Steps through the environment that on the first call will run the `passive_env_step_check`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchecked_step\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}